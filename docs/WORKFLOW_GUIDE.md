# Kaggle ã‚³ãƒ³ãƒšã®é€²ã‚æ–¹ã‚¬ã‚¤ãƒ‰

ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã¯ã€ç’°å¢ƒæ§‹ç¯‰å¾Œã® **Kaggle ã‚³ãƒ³ãƒšã®é€²ã‚æ–¹** ã¨ **ã‚¸ãƒ§ãƒ–æŠ•å…¥ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹** ã‚’è§£èª¬ã—ã¾ã™ã€‚

---

## ğŸ“Š Kaggle ã‚³ãƒ³ãƒšã®åŸºæœ¬ãƒ•ãƒ­ãƒ¼

```
1. EDAï¼ˆæ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿åˆ†æï¼‰
   â†“
2. ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ä½œæˆ
   â†“
3. ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
   â†“
4. ãƒ¢ãƒ‡ãƒ«æ”¹å–„ãƒ»ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
   â†“
5. ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
   â†“
6. æœ€çµ‚æå‡º
```

---

## ğŸ” Phase 1: EDAï¼ˆæ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿åˆ†æï¼‰

### ç›®çš„

- ãƒ‡ãƒ¼ã‚¿ã®å…¨ä½“åƒã‚’æŠŠæ¡ã™ã‚‹
- æ¬ æå€¤ãƒ»å¤–ã‚Œå€¤ã‚’ç¢ºèªã™ã‚‹
- ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®é–¢ä¿‚ã‚’ç†è§£ã™ã‚‹

### ã‚„ã‚Šæ–¹

```bash
# JupyterLab ã‚’èµ·å‹•ï¼ˆè¨ˆç®—ãƒãƒ¼ãƒ‰ã§ï¼‰
qrsh -q tsmall -l gpu=1 -l mem_req=16g -l h_vmem=16g
cd ~/kaggle/competitions/kaggle-s6e2-heart/docker
docker compose up
```

```python
# notebooks/00_eda_initial.ipynb ã§
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
train = pd.read_csv('/data/datasets/raw/train.csv')

# åŸºæœ¬æƒ…å ±
print(train.shape)
print(train.info())
print(train.describe())

# ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ†å¸ƒ
train['Heart Disease'].value_counts().plot(kind='bar')

# ç›¸é–¢è¡Œåˆ—
sns.heatmap(train.corr(), annot=True, cmap='coolwarm')
```

### ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ

- [ ] ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºï¼ˆè¡Œæ•°ãƒ»åˆ—æ•°ï¼‰ã‚’ç¢ºèª
- [ ] å„åˆ—ã®ãƒ‡ãƒ¼ã‚¿å‹ã‚’ç¢ºèª
- [ ] æ¬ æå€¤ã®æœ‰ç„¡ã‚’ç¢ºèª
- [ ] ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã®åˆ†å¸ƒã‚’ç¢ºèªï¼ˆä¸å‡è¡¡ã‹ã©ã†ã‹ï¼‰
- [ ] æ•°å€¤å¤‰æ•°ã®åˆ†å¸ƒã‚’ç¢ºèª
- [ ] ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯æ•°ã‚’ç¢ºèª

---

## ğŸ¯ Phase 2: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ä½œæˆ

### ç›®çš„

- æœ€å°é™ã®å‰å‡¦ç†ã§å‹•ããƒ¢ãƒ‡ãƒ«ã‚’ä½œã‚‹
- æå‡ºã¾ã§ã®ä¸€é€£ã®æµã‚Œã‚’ç¢ºèªã™ã‚‹
- æ”¹å–„ã®åŸºæº–ç‚¹ã‚’ä½œã‚‹

### ã‚„ã‚Šæ–¹

```bash
cd ~/kaggle/competitions/kaggle-s6e2-heart

# ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å­¦ç¿’ã‚’å®Ÿè¡Œ
qsub scripts/submit_job.sh src/train.py

# ãƒ­ã‚°ã‚’ç¢ºèª
cat logs/kaggle-run.o*

# æå‡º
cp ~/kaggle_data/outputs/submission_v1.csv data/output/
./scripts/submit.sh data/output/submission_v1.csv "LightGBM baseline v1"
```

### ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ

- [ ] CV ã‚¹ã‚³ã‚¢ã‚’è¨˜éŒ²ï¼ˆä¾‹: AUC 0.9552ï¼‰
- [ ] Public LB ã‚¹ã‚³ã‚¢ã‚’è¨˜éŒ²
- [ ] CV ã¨ LB ã®ä¹–é›¢ã‚’ç¢ºèªï¼ˆå¤§ããªä¹–é›¢ã¯éå­¦ç¿’ã®å…†å€™ï¼‰

---

## ğŸ› ï¸ Phase 3: ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°

### ç›®çš„

- ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ç²¾åº¦ã‚’å‘ä¸Šã•ã›ã‚‹ç‰¹å¾´é‡ã‚’ä½œæˆ
- ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã‚’æ´»ç”¨

### ã‚ˆãä½¿ã†æ‰‹æ³•

#### 1. æ•°å€¤å¤‰æ•°ã®å¤‰æ›

```python
# å¯¾æ•°å¤‰æ›ï¼ˆæ­ªã‚“ã åˆ†å¸ƒã‚’æ­£è¦åŒ–ï¼‰
df['log_age'] = np.log1p(df['age'])

# äºŒä¹—ãƒ»å¹³æ–¹æ ¹
df['age_squared'] = df['age'] ** 2
df['age_sqrt'] = np.sqrt(df['age'])

# ãƒ“ãƒ‹ãƒ³ã‚°ï¼ˆé€£ç¶šå€¤ã‚’ã‚«ãƒ†ã‚´ãƒªåŒ–ï¼‰
df['age_bin'] = pd.cut(df['age'], bins=[0, 30, 50, 70, 100], labels=['young', 'middle', 'senior', 'elderly'])
```

#### 2. äº¤äº’ä½œç”¨ç‰¹å¾´é‡

```python
# 2ã¤ã®ç‰¹å¾´é‡ã®ç©
df['age_x_cholesterol'] = df['age'] * df['cholesterol']

# æ¯”ç‡
df['bp_ratio'] = df['systolic_bp'] / (df['diastolic_bp'] + 1)
```

#### 3. é›†ç´„ç‰¹å¾´é‡

```python
# ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã®çµ±è¨ˆé‡
df['mean_age_by_sex'] = df.groupby('sex')['age'].transform('mean')
df['age_diff_from_mean'] = df['age'] - df['mean_age_by_sex']
```

#### 4. ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°

```python
# Label Encoding
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df['sex_encoded'] = le.fit_transform(df['sex'])

# Target Encodingï¼ˆCVå†…ã§è¡Œã†ã“ã¨ï¼ãƒªãƒ¼ã‚¯æ³¨æ„ï¼‰
from sklearn.model_selection import KFold
# ... å®Ÿè£…ã¯çœç•¥
```

### ç‰¹å¾´é‡ã‚’è¿½åŠ ã—ãŸã‚‰

```bash
# src/preprocessing.py ã«é–¢æ•°ã‚’è¿½åŠ 
# src/train.py ã§å‘¼ã³å‡ºã—
# ã‚¸ãƒ§ãƒ–ã‚’æŠ•å…¥ã—ã¦æ¤œè¨¼
qsub -N feat-v2 scripts/submit_job.sh src/train.py
```

---

## ğŸ“ˆ Phase 4: ãƒ¢ãƒ‡ãƒ«æ”¹å–„ãƒ»ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

### 1. ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆOptunaï¼‰

```python
import optuna

def objective(trial):
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
        'max_depth': trial.suggest_int('max_depth', 3, 10),
        'num_leaves': trial.suggest_int('num_leaves', 20, 150),
        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),
        'subsample': trial.suggest_float('subsample', 0.5, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
    }
    
    # CV ã§è©•ä¾¡
    cv_score = cross_validate(params)
    return cv_score

study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=100)

print(f"Best params: {study.best_params}")
print(f"Best score: {study.best_value}")
```

### 2. ä»–ã®ãƒ¢ãƒ‡ãƒ«ã‚’è©¦ã™

| ãƒ¢ãƒ‡ãƒ« | ç‰¹å¾´ | GPUå¯¾å¿œ |
|--------|------|---------|
| LightGBM | é«˜é€Ÿã€ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ãŒè‰¯ã„ | âŒï¼ˆCPUç‰ˆï¼‰ |
| XGBoost | ç²¾åº¦ãŒé«˜ã„ã€GPUå¯¾å¿œ | âœ… |
| CatBoost | ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã«å¼·ã„ã€GPUå¯¾å¿œ | âœ… |
| Neural Network | å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã«å¼·ã„ | âœ… |

```python
# XGBoostï¼ˆGPUç‰ˆï¼‰
import xgboost as xgb

params = {
    'tree_method': 'gpu_hist',  # GPU ã‚’ä½¿ç”¨
    'gpu_id': 0,
    'objective': 'binary:logistic',
    'eval_metric': 'auc',
}

# CatBoostï¼ˆGPUç‰ˆï¼‰
from catboost import CatBoostClassifier

model = CatBoostClassifier(
    task_type='GPU',
    devices='0',
    iterations=1000,
    learning_rate=0.05,
)
```

### 3. Cross Validation ã®å·¥å¤«

```python
# Stratified K-Foldï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
from sklearn.model_selection import StratifiedKFold
kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Group K-Foldï¼ˆã‚°ãƒ«ãƒ¼ãƒ—å˜ä½ã§åˆ†å‰²ï¼‰
from sklearn.model_selection import GroupKFold
gkf = GroupKFold(n_splits=5)

# Time Series Splitï¼ˆæ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ï¼‰
from sklearn.model_selection import TimeSeriesSplit
tscv = TimeSeriesSplit(n_splits=5)
```

---

## ğŸ­ Phase 5: ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«

### 1. å˜ç´”å¹³å‡

```python
# è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã‚’å¹³å‡
pred_lgb = model_lgb.predict_proba(X_test)[:, 1]
pred_xgb = model_xgb.predict_proba(X_test)[:, 1]
pred_cat = model_cat.predict_proba(X_test)[:, 1]

final_pred = (pred_lgb + pred_xgb + pred_cat) / 3
```

### 2. é‡ã¿ä»˜ãå¹³å‡

```python
# CV ã‚¹ã‚³ã‚¢ã«åŸºã¥ã„ã¦é‡ã¿ä»˜ã‘
weights = [0.4, 0.35, 0.25]  # LGB, XGB, CAT
final_pred = weights[0] * pred_lgb + weights[1] * pred_xgb + weights[2] * pred_cat
```

### 3. Stacking

```python
from sklearn.ensemble import StackingClassifier

estimators = [
    ('lgb', lgb_model),
    ('xgb', xgb_model),
    ('cat', cat_model),
]

stacking = StackingClassifier(
    estimators=estimators,
    final_estimator=LogisticRegression(),
    cv=5,
)
```

---

## ğŸ–¥ï¸ ã‚¸ãƒ§ãƒ–æŠ•å…¥ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### åŸºæœ¬çš„ãªã‚¸ãƒ§ãƒ–æŠ•å…¥

```bash
cd ~/kaggle/competitions/kaggle-s6e2-heart

# å­¦ç¿’ã‚¸ãƒ§ãƒ–ã‚’æŠ•å…¥
qsub scripts/submit_job.sh src/train.py

# ã‚¸ãƒ§ãƒ–åã‚’ä»˜ã‘ã¦æŠ•å…¥ï¼ˆãƒ­ã‚°ã®è­˜åˆ¥ã«ä¾¿åˆ©ï¼‰
qsub -N lgb-v2 scripts/submit_job.sh src/train.py

# å¼•æ•°ã‚’æ¸¡ã™
qsub scripts/submit_job.sh src/train.py --model xgboost --n_estimators 1000
```

### ã‚¸ãƒ§ãƒ–ã®ç›£è¦–

```bash
# ã‚¸ãƒ§ãƒ–ã®çŠ¶æ…‹ç¢ºèª
qstat

# è©³ç´°æƒ…å ±
qstat -j <ã‚¸ãƒ§ãƒ–ID>

# ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ­ã‚°ç›£è¦–
tail -f logs/kaggle-run.o<ã‚¸ãƒ§ãƒ–ID>

# ã‚¸ãƒ§ãƒ–ã®å‰Šé™¤
qdel <ã‚¸ãƒ§ãƒ–ID>
```

### è¤‡æ•°å®Ÿé¨“ã‚’ä¸¦åˆ—å®Ÿè¡Œ

```bash
# æ–¹æ³•1: è¤‡æ•°ã® qsub ã‚’æŠ•å…¥
qsub -N lgb-lr01 scripts/submit_job.sh src/train.py --learning_rate 0.1
qsub -N lgb-lr005 scripts/submit_job.sh src/train.py --learning_rate 0.05
qsub -N lgb-lr001 scripts/submit_job.sh src/train.py --learning_rate 0.01

# æ–¹æ³•2: ã‚¢ãƒ¬ã‚¤ã‚¸ãƒ§ãƒ–ã‚’ä½¿ç”¨
qsub scripts/job_array.sh
```

### ãƒªã‚½ãƒ¼ã‚¹ã®èª¿æ•´

```bash
# ãƒ¡ãƒ¢ãƒªã‚’å¢—ã‚„ã™
qsub -l mem_req=32g -l h_vmem=32g scripts/submit_job.sh src/train.py

# GPU ã‚’ 2 æšä½¿ã†
qsub -l gpu=2 scripts/submit_job.sh src/train.py

# åˆ¥ã®ã‚­ãƒ¥ãƒ¼ã‚’ä½¿ã†
qsub -q tlarge scripts/submit_job.sh src/train.py
```

### ãƒ­ã‚°ã®æ•´ç†

```bash
# ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç¢ºèª
ls -la logs/

# å¤ã„ãƒ­ã‚°ã‚’å‰Šé™¤
rm logs/kaggle-run.o*
rm logs/kaggle-run.e*

# ç‰¹å®šã®ã‚¸ãƒ§ãƒ–ã®ãƒ­ã‚°ã‚’ç¢ºèª
cat logs/kaggle-run.o199138
```

---

## ğŸ“ å®Ÿé¨“ç®¡ç†ã®ã‚³ãƒ„

### 1. å®Ÿé¨“ãƒ­ã‚°ã‚’è¨˜éŒ²ã™ã‚‹

| æ—¥ä»˜ | å®Ÿé¨“å | å¤‰æ›´å†…å®¹ | CV Score | LB Score | ãƒ¡ãƒ¢ |
|------|--------|----------|----------|----------|------|
| 2/6 | baseline | LightGBM æ•°å€¤ã®ã¿ | 0.9552 | 0.9548 | åˆå›æå‡º |
| 2/7 | feat-v1 | äº¤äº’ä½œç”¨ç‰¹å¾´é‡è¿½åŠ  | 0.9580 | - | +0.0028 |
| 2/8 | xgb-v1 | XGBoost ã«å¤‰æ›´ | 0.9575 | - | LGBã‚ˆã‚Šå°‘ã—ä½ã„ |

### 2. ã‚³ãƒ¼ãƒ‰ã‚’ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã™ã‚‹

```bash
# å®Ÿé¨“ã”ã¨ã«ã‚³ãƒŸãƒƒãƒˆ
git add src/train.py
git commit -m "Add interaction features, CV=0.9580"
git push origin main
```

### 3. ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã™ã‚‹

```python
# ãƒ¢ãƒ‡ãƒ«ä¿å­˜
model.booster_.save_model(f'/data/models/lgbm_v2_fold{fold}.txt')

# äºˆæ¸¬çµæœã‚‚ä¿å­˜
np.save('/data/outputs/oof_preds_v2.npy', oof_preds)
np.save('/data/outputs/test_preds_v2.npy', test_preds)
```

---

## ğŸš€ ã‚¹ã‚³ã‚¢ã‚’ä¸Šã’ã‚‹ãŸã‚ã®ãƒ’ãƒ³ãƒˆ

### 1. ãƒ‡ãƒ¼ã‚¿ã‚’ã‚ˆãè¦‹ã‚‹

- EDA ã«æ™‚é–“ã‚’ã‹ã‘ã‚‹
- å¤–ã‚Œå€¤ãƒ»ç•°å¸¸å€¤ã‚’ç¢ºèª
- ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®é–¢ä¿‚ã‚’å¯è¦–åŒ–

### 2. æ¤œè¨¼ã‚’ä¿¡ã˜ã‚‹

- CV ã‚¹ã‚³ã‚¢ã¨ LB ã‚¹ã‚³ã‚¢ã®ç›¸é–¢ã‚’ç¢ºèª
- éå­¦ç¿’ã«æ³¨æ„ï¼ˆCV >> LB ã¯å±é™ºä¿¡å·ï¼‰
- å®‰å®šã—ãŸ CV è¨­è¨ˆã‚’å¿ƒãŒã‘ã‚‹

### 3. ã‚·ãƒ³ãƒ—ãƒ«ã‹ã‚‰å§‹ã‚ã‚‹

- æœ€åˆã¯å˜ç´”ãªãƒ¢ãƒ‡ãƒ«ã§
- å¾ã€…ã«è¤‡é›‘ã«ã—ã¦ã„ã
- æ”¹å–„ãŒè¦‹ã‚‰ã‚Œãªããªã£ãŸã‚‰æ¬¡ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¸

### 4. Discussionãƒ»Notebook ã‚’èª­ã‚€

- Kaggle ã® Discussion ã§æƒ…å ±åé›†
- å…¬é–‹ Notebook ã‹ã‚‰ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’å¾—ã‚‹
- ãŸã ã—ã€ãã®ã¾ã¾ä½¿ã†ã®ã§ã¯ãªãç†è§£ã—ã¦å¿œç”¨

### 5. ãƒãƒ¼ãƒ ã§å”åŠ›

- é€±æ¬¡ãƒŸãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã§é€²æ—å…±æœ‰
- ã†ã¾ãã„ã£ãŸæ‰‹æ³•ã‚’å…±æœ‰
- ç•°ãªã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’è©¦ã™

---

## ğŸ“š å‚è€ƒãƒªãƒ³ã‚¯

- [Kaggle Learn](https://www.kaggle.com/learn) - ç„¡æ–™ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«
- [Kaggle Courses](https://www.kaggle.com/learn) - æ©Ÿæ¢°å­¦ç¿’ã‚³ãƒ¼ã‚¹
- [Feature Engineering Book](https://www.oreilly.com/library/view/feature-engineering-for/9781491953235/) - ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®æ›¸ç±
- [Optuna Documentation](https://optuna.readthedocs.io/) - ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

---

**Good luck with your Kaggle journey! ğŸ†**
